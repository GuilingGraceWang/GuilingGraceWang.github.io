<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
  <head profile="http://gmpg.org/xfn/11">
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
 
<!-- Meta Images -->
    <!--<link rel="shortcut icon" href="http://iqua.ece.toronto.edu/wp-content/themes/iBlogPro/core/images/favicon-iqua.ico" type="image/x-icon" /> -->
    <link rel="shortcut icon" href="img/water.jpg" type="image/x-icon" />   
<!-- Title and External Script Integration -->
            <title>Guiling Wang</title>
        
<!-- Stylesheets -->
    <link rel="stylesheet" href="css/reset.css" type="text/css" media="screen" />
    <link rel="stylesheet" href="css/wp_core.css" type="text/css" media="screen" />
    <link rel="stylesheet" href="css/style.css" type="text/css" media="screen" />
    <link rel="stylesheet" href="css/pro.css" type="text/css" media="screen" />             

<style type="text/css"> 
    #nav ul{width: 950px;} 
    body{}
</style>

    </head>
<body class="page page-id-22 page-parent page-template page-template-page-highlight-full page-template-page-highlight-full-php">
 
<div id="page" class="fix" style="">
  <div id="wrapper" class="fix" >
    <div id="header" class="fix">
                    <a href="http://cs.njit.edu">
            <img class="headerimage" src="img/department.jpg" alt="NJIT"/>
            </a>
    </div><!-- /header -->
    <div id="nav" class="fix">
    <ul class="fix dropdown">
        <li class="page_item navfirst">
        
            <a class="home" href="index.html" title="Home" style="background-image: url('img/home-icon-trans.jpg');">
                Home    
            </a>
        </li>
<li class="page_item navfirst"><a href="research.html">Research</a>
</li>
<li class="page_item page-item-6 page_item_has_children"><a href="publications.html">Publications</a>
</li>
<li class="page_item page-item-29 page_item_has_children"><a href="teaching.html">Teaching</a>
</li>
<li class="page_item navfirst"><a href="service.html">Service</a>
</li>
<li class="page_item navfirst"><a href="people.html">People</a>
</li>
<!-- <li class="page_item navfirst"><a href="contact.html">Contact</a>
</li> -->
</ul>
    </div><!-- /nav -->
    <div id="container" class="fix ">
 
<div id="content">
 
            
            
    <div class="postwrap fix">
        
        <div class="post-607 page type-page status-publish hentry" id="post-607">
                            
                <div class="copy fix">
                                        <div class="textcontent">

<!-- <p>
<h3 class="fsub"><a href="https://scholar.google.com/citations?user=PJOGQc4AAAAJ&hl=en&authuser=1"><b>Google Scholar</b></a></h3>
</p> -->

<!--<p>
<h3 class="fsub"><a name="Research Interest" ></a><b>Research Interest</b></h3>
</p>
<ul>
<li><font face="Verdana" style="font-size: 10pt">Applied deep learning and machine learning</font></li>
<li><font face="Verdana" style="font-size: 10pt">Blockchain technologies</font></li>
<li><font face="Verdana" style="font-size: 10pt">FinTech (<a href="https://fintechlab-njit.netlify.app">FinTech Lab</a>)</font></li>
<li><font face="Verdana" style="font-size: 10pt">Mobile computing and IoT</font></li>
<li><font face="Verdana" style="font-size: 10pt">Intelligent transportation Systems</font></li>
</ul>

-->

<p>
    <h3 class="fsub"><a name="Research Projects"></a><b>Research Projects on Computer Vision</b></h3>
</p>
<ul>
    <li><a href="#project1" font face="Verdana" style="font-size: 10pt">Reverse Pass-Through VR with Full Head Avatars</a></li>
    <li><a href="#project2" font face="Verdana" style="font-size: 10pt">High Resolution Solar Image Generation using Generative Adversarial Networks</a></li>
    <li><a href="#project3" font face="Verdana" style="font-size: 10pt">HierGAN: GAN-Based Hierarchical Model for Combined RGB and Depth Inpainting</a></li>
    <li><a href="#project4" font face="Verdana" style="font-size: 10pt">Attentive Partial Convolution for RGBD Inpainting  </a></li>
    
	<!-- <li><a href="#project2" font face="Verdana" style="font-size: 10pt">GAN-NeRF for Hi-Resolution Novel View Synthesis</a></li>
	<li><a href="#project4" font face="Verdana" style="font-size: 10pt">Reverse Pass-through VR for Enhanced Social Interaction </a></li> 
     -->
    
</ul>

<!-- Project Details -->

<div id="project1" class="project-detail">
    <div class="description">
        <h4>Reverse Pass-Through VR with Full Head Avatars</h4>
        <p>Virtual Reality (VR) headsets are becoming increasingly integral to today’s digital ecosystem, yet they introduce a critical challenge by obscuring users' eyes and portions of their faces, which diminishes visual connections and may contribute to social isolation. To address this, we introduce RevAvatar, a device-agnostic framework designed to mitigate VR-induced isolation and enhance user interaction within both virtual and physical environments.
RevAvatar enables real-time reverse pass-through by projecting users' eyes and facial expressions onto VR headsets. The end-to-end system leverages deep learning and computer vision to reconstruct 2D facial images from partially observed eye and lower-face regions captured by VR headsets, while also generating accurate 3D head avatars using a one-shot approach for VR meetings and other immersive experiences. Its real-time capabilities ensure rapid inference and seamless deployment across mainstream VR devices.
To further support research in this area, we introduce VR-Face, a novel dataset comprising 70,000 samples that simulate complex VR conditions, including diverse occlusions, lighting variations, and differing levels of visibility.

        </p>
    </div>
    <div class="project-image">
        <img src="img/Reverse_Pass-Through.png" alt="margin_trader">
    </div>
</div>

<div id="project2" class="project-detail">
    <div class="description">
        <h4>High Resolution Solar Image Generation using Generative Adversarial Networks</h4>
        <p>We applied Deep Learning algorithm known as Generative Adversarial Networks (GANs) to perform solar image-to-image translation. That is, from Solar Dynamics Observatory (SDO)/Helioseismic and Magnetic Imager(HMI) line of sight magnetogram images to SDO/Atmospheric Imaging Assembly(AIA) 0304-Å images. Ultraviolet (UV)/Extreme Ultraviolet(EUV) observations like the SDO/AIA0304-Å images were only made available to scientists in the late 1990s even though magnetic field observations like the SDO/HMI have been available since the 1970s. Therefore by leveraging Deep Learning algorithms like GANs we can give scientists access to complete datasets for analysis. For generating high resolution solar images we use the Pix2PixHD and Pix2Pix algorithms. The Pix2PixHD algorithm was specifically designed for high resolution image generation tasks, and the Pix2Pix algorithm is by far the most widely used image to image translation algorithm. For training and testing we used the data for the year 2012, 2013 and 2014. The results show that our deep learning models are capable of generating high resolution(1024 x 1024 pixels) AIA0304 images from HMI magnetograms. Specifically, the pixel-to-pixel Pearson Correlation Coefficient of the images generated by Pix2PixHD and original images is as high as 0.99. The number is 0.962 if Pix2Pix is used to generate images. The results we get for our Pix2PixHD model is better than the results obtained by previous works done by others to generate AIA0304 images. Thus, we can use these models to generate AIA0304 images when the AIA0304 data is not available which can be used for understanding space weather and giving researchers the capability to predict solar events such as Solar Flares and Coronal Mass Ejections. As far as we know, our work is the first attempt to leverage Pix2PixHD algorithm for SDO/HMI to SDO/AIA0304 image-to-image translation.

        </p>
    </div>
    <div class="project-image">
        <img src="img/Solar.png" alt="margin_trader">
    </div>
</div>

<div id="project3" class="project-detail">
    <div class="description">
        <h4>HierGAN: GAN-Based Hierarchical Model for Combined RGB and Depth Inpainting</h4>
        <p>Inpainting involves filling in missing pixels or areas in an image, a crucial technique employed in Mixed Reality environments for various applications, particularly in Diminished Reality (DR) where content is removed from a user’s visual environment. Existing methods rely on digital replacement techniques, which necessitate multiple cameras and incur high costs. AR devices and smartphones use ToF depth sensors to capture scene depth maps aligned with RGB images. Despite their speed and affordability, ToF cameras create imperfect depth maps with missing pixels. To address these challenges, we propose Hierarchical Inpainting GAN (HierGAN), a novel approach comprising three GANs in a hierarchical fashion for RGBD inpainting. EdgeGAN and LabelGAN inpaint masked edge and segmentation label images, respectively, while CombinedRGBD-GAN combines their latent representation outputs and performs RGB and depth inpainting. Edge images and particularly segmentation label images as auxiliary inputs significantly enhance inpainting performance by providing complementary context and hierarchical optimization. We believe we are the first to incorporate label images into the inpainting process. Unlike previous approaches requiring multiple sequential models and separate outputs, our work operates in an end-to-end manner, training all three models simultaneously and hierarchically. Specifically, EdgeGAN and LabelGAN are first optimized separately and further optimized within CombinedRGBD-GAN to enhance inpainting quality. Experiments demonstrate that HierGAN works seamlessly and achieves overall superior performance compared to existing approaches.
        </p>
    </div>
    <div class="project-image">
        <img src="img/HierGAN.png" alt="margin_trader">
    </div>
</div>

<div id="project4" class="project-detail">
    <div class="description">
        <h4>Attentive Partial Convolution for RGBD Inpainting</h4>
        <p>The process of Inpainting, which involves reconstructing missing pixels within images, plays a pivotal role in refining image processing and augmenting reality (AR) encounters. This study tackles three prominent hurdles in AR technology: diminished reality (DR), which entails removing undesired elements from the user's view; the latency issue in AR head-mounted displays leading to pixel gaps; and the flaws in depth maps generated by Time-of-Flight (ToF) sensors in AR devices. These obstacles compromise the authenticity and engagement of AR experiences by affecting both the texture and geometric accuracy of digital content.
            We introduce an innovative Partial Convolution-based framework tailored for RGBD (Red, Green, Blue, Depth) image inpainting, proficient in simultaneously reinstating missing pixels in both the color (RGB) and depth dimensions of an image. Unlike traditional methods that primarily concentrate on RGB inpainting, our approach integrates depth data, crucial for lifelike AR applications, by restoring both the spatial structure and visual details. This dual restoration ability is paramount for crafting immersive AR experiences, ensuring seamless amalgamation of virtual and real-world elements.
            Our contributions encompass the refinement of an advanced Partial Convolution model, incorporating attentive normalization and an updated loss function, which surpasses existing models in accuracy and realism in inpainting endeavors. 
        </p>
    </div>
    <div class="project-image">
        <img src="img/attnpconvmodel.png" alt="margin_trader">
    </div>
</div>


<!-- 
<div id="project2" class="project-detail">
    <div class="description">
        <h4>GAN-NeRF for Hi-Resolution Novel View Synthesis </h4>
        <p>
            Introducing GAN-NeRF, an innovative approach to 3D scene reconstruction that merges the capabilities of Neural Radiance Fields (NeRF) and Generative Adversarial Networks (GANs). Our method capitalizes on the distinct advantages of each model to produce high resolution 3D reconstructions even with limited input data. Recognizing the challenges of acquiring numerous images with precise camera parameters, we propose a strategy that harnesses a small set of such images to train three foundational NeRF-based models (NeRF, Mip-NeRF, and Mip-NeRF360). These models serve as the backbone, followed by a refinement phase employing a conditional Generative Adversarial Network (GAN) model.
            While NeRFs excel at generating new perspectives, they often exhibit flaws and artifacts without extensive training data. To combat this, we integrate the GAN model renowned for its proficiency in crafting realistic textures and intricate details, thus significantly enhancing the fidelity of the 3D reconstructions. GAN-NeRF represents a comprehensive and lifelike 3D model, seamlessly blending NeRFs' structural comprehension with GANs' textural richness.
            Extensive experimentation validates the efficacy of GAN-NeRF, showcasing its ability to generate high-quality 3D reconstructions with minimal input requirements. This makes it particularly promising for scenarios where gathering abundant high-quality images with precise camera parameters proves challenging or unfeasible. Notably, the refined images and quantitative results from GAN-NeRF consistently outperform other NeRF-based methods across various datasets, exhibiting superior SSIM, PSNR, and lower LPIPS and KID metric values.
        </p>
    </div>
    <div class="project-image">
        <img src="img/GANNERF.png" alt="signal_control">
    </div>
</div>

<div id="project3" class="project-detail">
    <div class="description">
        <h4>Reverse Pass-through VR for Enhanced Social Interaction</h4>
        <p>
            Virtual Reality (VR) headsets have become increasingly indispensable in today's digital landscape. However, they pose a significant challenge by blocking users' eyes, disrupting visual connections and fostering social isolation. Our primary goal is to combat this issue by introducing a groundbreaking Reverse Pass-through framework, specifically tailored for VR headsets. This framework enables users to display their eyes to their surroundings via an outward-facing screen.
            Our innovative approach incorporates left and right eye images to fully restore eye features and capture subtle facial expressions. To accomplish this, we have curated a new VR-Eyes image dataset, mimicking captures from infrared cameras embedded within the headset. 
            Crucially, our proposed methodology employs lightweight models, facilitating easy deployment on VR devices for quick inference. We believe this initiative represents one of the pioneering efforts to address the issue of social isolation induced by VR headsets through this unique approach.
        </p>
    </div>
    <div class="project-image">
        <img src="img/mysightOutputs.png" alt="ts_mixer">
    </div>
</div> -->

 
</body>
</html>
<!-- Performance optimized by W3 Total Cache. Learn more: http://www.w3-edge.com/wordpress-plugins/
 
Page Caching using disk: enhanced
 
 Served from: iqua.ece.toronto.edu @ 2015-02-14 01:07:43 by W3 Total Cache -->
